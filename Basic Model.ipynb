{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import json\n",
    "from sklearn.utils import shuffle\n",
    "import copy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sklearn.ensemble\n",
    "import sklearn as skl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCooffending():\n",
    "    co_offending_table = pd.read_csv('./Cooffending.csv')\n",
    "    \n",
    "    # Remove duplicate rows\n",
    "    co_offending_table.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Format the date column as a python datetime object\n",
    "    co_offending_table['Date'] = co_offending_table.Date.apply(lambda x: datetime.datetime.strptime(x, '%m/%d/%Y'))\n",
    "    co_offending_table['year']=co_offending_table.Date.dt.year\n",
    "    co_offending_table['month']=co_offending_table.Date.dt.month\n",
    "    # Add a column for the number of arrests of each offender\n",
    "    co_offending_table['ArrestCount'] = co_offending_table.groupby('NoUnique')['NoUnique'].transform('count')\n",
    "    \n",
    "    # Get the right datatype \n",
    "    co_offending_table.SeqE = co_offending_table.SeqE.astype('category')\n",
    "    co_offending_table.SEXE = co_offending_table.SEXE.astype('category')\n",
    "    co_offending_table.NCD1 = co_offending_table.NCD1.astype('category')\n",
    "    co_offending_table.NCD2 = co_offending_table.NCD2.astype('category')\n",
    "    co_offending_table.NCD3 = co_offending_table.NCD3.astype('category')\n",
    "    co_offending_table.NCD4 = co_offending_table.NCD4.astype('category')\n",
    "    co_offending_table.MUN = co_offending_table.MUN.astype('category')\n",
    "    co_offending_table.ED1 = co_offending_table.ED1.astype('category')\n",
    "    return co_offending_table\n",
    "\n",
    "co_offending_table=getCooffending()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_second_arrests(df):\n",
    "    if len(df) == 1: \n",
    "        return None\n",
    "    else:\n",
    "        return df.iloc[1]\n",
    "    \n",
    "def process_third_arrests(df):\n",
    "    if len(df) == 2: \n",
    "        return None\n",
    "    else:\n",
    "        return df.iloc[2]\n",
    "    \n",
    "def process_seq_numbers(x):\n",
    "    if np.isnan(x):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def build_table_of_first_two_arrests(co_offending):\n",
    "    # first sort by offender and date\n",
    "    co_offending = co_offending.sort_values(by=['NoUnique', 'Date'])\n",
    "    # this gets the first arrest of each offender (which is gauranteed to exist)\n",
    "    print('sorted')\n",
    "    first_arrest = co_offending.groupby('NoUnique').apply(lambda x: x.iloc[0])\n",
    "    first_arrest.columns = ['first_arrest_'+str(x) for x in first_arrest.columns]\n",
    "    print(first_arrest)\n",
    "    # this gets the second arrest of each offender or NaN if the offender does not exist\n",
    "    second_arrests = co_offending.groupby('NoUnique').apply(process_third_arrests)\n",
    "    second_arrests.columns = ['second_arrest_'+str(x) for x in second_arrests.columns]\n",
    "    print(second_arrests)\n",
    "    # combine these two dataframe \n",
    "    first_and_second_arrest_data = pd.merge(first_arrest, second_arrests, how='outer', left_index=True, right_index=True)\n",
    "    # add a column with a binary variable 0/1 for whether arrested again\n",
    "    first_and_second_arrest_data['arrested_again'] = first_and_second_arrest_data.second_arrest_NoUnique.apply(process_seq_numbers)\n",
    "    return first_and_second_arrest_data\n",
    "\n",
    "def build_table_of_second_and_third_arrests(co_off):\n",
    "    # first sort by offender and date\n",
    "    co_offending=co_off[co_off.ArrestCount>=2]\n",
    "    co_offending = co_offending.sort_values(by=['NoUnique', 'Date'])\n",
    "    # this gets the second arrest of each offender (which is gauranteed to exist after filter)\n",
    "    print('sorted')\n",
    "    first_arrest = co_offending.groupby('NoUnique').apply(lambda x: x.iloc[1])\n",
    "    first_arrest.columns = ['first_arrest_'+str(x) for x in first_arrest.columns]\n",
    "    print(first_arrest)\n",
    "    # this gets the second arrest of each offender or NaN if the offender does not exist\n",
    "    second_arrests = co_offending.groupby('NoUnique').apply(process_third_arrests)\n",
    "    second_arrests.columns = ['second_arrest_'+str(x) for x in second_arrests.columns]\n",
    "    print(second_arrests)\n",
    "    # combine these two dataframe \n",
    "    first_and_second_arrest_data = pd.merge(first_arrest, second_arrests, how='outer', left_index=True, right_index=True)\n",
    "    # add a column with a binary variable 0/1 for whether arrested again\n",
    "    first_and_second_arrest_data['arrested_again'] = first_and_second_arrest_data.second_arrest_NoUnique.apply(process_seq_numbers)\n",
    "    return first_and_second_arrest_data\n",
    "\n",
    "def readTrainingData(filename):\n",
    "    training_data = pd.read_csv(filename)\n",
    "    \n",
    "    def addMajorCrimeCategory(train):\n",
    "        mapper=pd.read_csv('crimeType.csv')\n",
    "        crimeTypeDict={' ':'x'}\n",
    "        def getDict(row):\n",
    "            crimeTypeDict[str(row.NCD)]=row.Type\n",
    "        mapper.apply(getDict,1)\n",
    "        crimeTypeDict.keys()\n",
    "        train['first_arrest_crimeType']=train.first_arrest_NCD1.apply(lambda x : crimeTypeDict[x]).astype('category')\n",
    "    \n",
    "    addMajorCrimeCategory(training_data)\n",
    "    training_data['age_cat']=pd.qcut(training_data.first_arrest_Naissance,10,labels=False).astype('category')\n",
    "    training_data['arrestedWithAdult']=(training_data['first_arrest_Adultes']>=2).astype(int)\n",
    "    training_data['arrestedWithYouth']=np.sign(training_data['first_arrest_Jeunes'])\n",
    "    training_data['arrestedWithSomeone']=np.sign(training_data['first_arrest_Adultes']+training_data['first_arrest_Jeunes'])\n",
    "    training_data['first_arrest_Date']=pd.to_datetime(training_data.first_arrest_Date)\n",
    "    training_data['first_arrest_year']=training_data.first_arrest_Date.dt.year\n",
    "    # format data\n",
    "    training_data.first_arrest_SeqE = training_data.first_arrest_SeqE.astype('category')\n",
    "    training_data.first_arrest_SEXE = training_data.first_arrest_SEXE.astype('category')\n",
    "    training_data.first_arrest_NCD1 = training_data.first_arrest_NCD1.astype('category')\n",
    "    training_data.first_arrest_NCD2 = training_data.first_arrest_NCD2.astype('category')\n",
    "    training_data.first_arrest_NCD3 = training_data.first_arrest_NCD3.astype('category')\n",
    "    training_data.first_arrest_NCD4 = training_data.first_arrest_NCD4.astype('category')\n",
    "    training_data.first_arrest_MUN = training_data.first_arrest_MUN.astype('category')\n",
    "    training_data.first_arrest_ED1 = training_data.first_arrest_ED1.astype('category')\n",
    "    training_data.second_arrest_SeqE = training_data.second_arrest_SeqE.astype('category')\n",
    "    training_data.second_arrest_SEXE = training_data.second_arrest_SEXE.astype('category')\n",
    "    training_data.second_arrest_NCD1 = training_data.second_arrest_NCD1.astype('category')\n",
    "    training_data.second_arrest_NCD2 = training_data.second_arrest_NCD2.astype('category')\n",
    "    training_data.second_arrest_NCD3 = training_data.second_arrest_NCD3.astype('category')\n",
    "    training_data.second_arrest_NCD4 = training_data.second_arrest_NCD4.astype('category')\n",
    "    training_data.second_arrest_MUN = training_data.second_arrest_MUN.astype('category')\n",
    "    training_data.second_arrest_ED1 = training_data.second_arrest_ED1.astype('category')\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw recidivism rate: 0.341884\n"
     ]
    }
   ],
   "source": [
    "#training_data = build_table_of_first_two_arrests(co_offending_table)\n",
    "# or read from csv\n",
    "training_data=readTrainingData('./basic_model_data.csv')\n",
    "print('raw recidivism rate: %f' % (1.0*sum(training_data.arrested_again)/len(training_data.arrested_again)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#arrest_data=build_table_of_second_and_third_arrests(co_offending_table)\n",
    "#arrest_data.to_csv('secondthirdarrest.csv')\n",
    "training_data=readTrainingData('secondthirdarrest.csv')\n",
    "print('raw recidivism rate: %f' % (1.0*sum(training_data.arrested_again)/len(training_data.arrested_again)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating network and getting network attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNetwork(df,year):\n",
    "    crimes=df.groupby('SeqE')\n",
    "    arrestedPerCrime=crimes.apply(len)\n",
    "    multiPersonCrime=arrestedPerCrime[arrestedPerCrime!=1].index\n",
    "    multiPersonCrime=df[df.SeqE.isin(multiPersonCrime)]\n",
    "    nodes=multiPersonCrime.NoUnique.unique()\n",
    "    print 'network has {} nodes'.format(len(nodes))\n",
    "    G=nx.Graph()\n",
    "    G.add_nodes_from(nodes)\n",
    "    multCrimes=multiPersonCrime.groupby('SeqE')\n",
    "    for seqe,crime in multCrimes:\n",
    "        people=list(crime.NoUnique)\n",
    "        for i in range(len(people)):\n",
    "            for j in range(i+1,len(people)):\n",
    "                G.add_edge(people[i],people[j])\n",
    "                \n",
    "    print 'network has {} edges'.format(len(G.edges()))\n",
    "    nx.write_gpickle(G,'G_crime_{}.pkl'.format(year))\n",
    "\n",
    "def getNetworks(df):\n",
    "    years=list(df.year.unique())\n",
    "    for year in years[-3:]:\n",
    "        dfyear=df[df.year<=year]\n",
    "        getNetwork(dfyear,year)\n",
    "\n",
    "def computeNetworkValues(G):\n",
    "    eigen=nx.eigenvector_centrality(G)\n",
    "    degree=G.degree()\n",
    "    closeness=nx.closeness_centrality(G)\n",
    "    between=nx.betweenness_centrality(G)\n",
    "    clus=nx.clustering(G)\n",
    "    return degree,eigen,between,closeness,clus\n",
    "\n",
    "def computeNetworksValues(df):\n",
    "    years=list(df.year.unique())\n",
    "    networkFeatures={}\n",
    "    for year in years:\n",
    "        networkFeatures[year]={}\n",
    "        G=nx.read_gpickle('G_crime_{}.pkl'.format(year))\n",
    "        networkFeatures[year]['degree'],networkFeatures[year]['eigen'],networkFeatures[year]['between'],networkFeatures[year]['closeness'],networkFeatures[year]['clus']=computeNetworkValues(G)\n",
    "        json.dump(networkFeatures, open('networkFeatures.json', 'wb'))\n",
    "\n",
    "def runForNetworkVal(co_offending_table,read=True):\n",
    "    if not read:\n",
    "        getNetworks(co_offending_table)\n",
    "    computeNetworksValues(co_offending_table)\n",
    "    \n",
    "#runForNetworkVal(co_offending_table,read=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting network attributes to prediction features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNetworkVar(row,networkFeatures,crimeDict,Gs):\n",
    "    year=row.first_arrest_year\n",
    "    G=Gs[year]\n",
    "    year=str(year)\n",
    "    eigen,degree,clus,closeness=0,0,0,0\n",
    "    crime=str(row.first_arrest_SeqE)\n",
    "    people=copy.deepcopy(crimeDict[crime])\n",
    "    offender=row.first_arrest_NoUnique\n",
    "    people.remove(offender)\n",
    "    networkFeat=networkFeatures[year]\n",
    "    for person in people:\n",
    "        try:\n",
    "            sperson=str(person)\n",
    "            eigen+=networkFeat['eigen'][sperson]\n",
    "            degree+=networkFeat['degree'][sperson]\n",
    "            clus+=networkFeat['clus'][sperson]\n",
    "            closeness+=networkFeat['closeness'][sperson]\n",
    "        except:\n",
    "            pass\n",
    "    count=0\n",
    "    for i in range(len(people)):\n",
    "        for j in range(i+1,len(people)):\n",
    "            count+=int(G.has_edge(people[i],people[j]))\n",
    "    total=len(people)*(len(people)-1)/2\n",
    "    if total!=0:\n",
    "        clus2=count*1.0/total\n",
    "    else:\n",
    "        clus2=0\n",
    "    return eigen,degree,clus,clus2,closeness\n",
    "\n",
    "def addNetworkFeatures(co_offending_table,df):\n",
    "    years=list(co_offending_table.year.unique())\n",
    "    Gs={}\n",
    "    for year in years:\n",
    "        Gs[year]=nx.read_gpickle('G_crime_{}.pkl'.format(year))\n",
    "    networkFeatures=json.load(open('networkFeatures.json','rb'))\n",
    "    crimeDict=json.load(open('crimeDict.json','rb'))\n",
    "    df2=pd.DataFrame(df.first_arrest_NoUnique)\n",
    "    df2['Neigen'],df2['Ndegree'],df2['Nclus'],df2['Nclus2'],df2['Ncloseness']=zip(*df[['first_arrest_year','first_arrest_NoUnique','first_arrest_SeqE']].apply(lambda x: getNetworkVar(x,networkFeatures,crimeDict,Gs),axis=1))\n",
    "    df2.to_csv('networkFeatures1_23.csv',index=False)\n",
    "    pass\n",
    "\n",
    "def getNetworkVar2(row,networkFeatures):\n",
    "    year=row.first_arrest_year\n",
    "    year=str(year)\n",
    "    offender=row.first_arrest_NoUnique\n",
    "    networkFeat=networkFeatures[year]\n",
    "    sperson=str(offender)\n",
    "    try:\n",
    "        eigen=networkFeat['eigen'][sperson]\n",
    "        degree=networkFeat['degree'][sperson]\n",
    "        clus=networkFeat['clus'][sperson]\n",
    "        closeness=networkFeat['closeness'][sperson]\n",
    "    except:\n",
    "        eigen=0\n",
    "        degree=0\n",
    "        clus=0\n",
    "        closeness=0\n",
    "        print (sperson)\n",
    "    return eigen,degree,clus,closeness\n",
    "\n",
    "def addNetworkFeatures2(co_offending_table,trainsub):\n",
    "    networkFeatures=json.load(open('networkFeatures.json','rb'))\n",
    "    df=pd.DataFrame(trainsub.first_arrest_NoUnique)\n",
    "    df['eigen'],df['degree'],df['clus'],df['closeness']=zip(*trainsub[['first_arrest_year','first_arrest_NoUnique','first_arrest_SeqE']].apply(lambda x: getNetworkVar2(x,networkFeatures),axis=1))\n",
    "    df.to_csv('networkFeatures2_23.csv',index=False)\n",
    "\n",
    "def getNetworkVar3(row,networkFeatures):\n",
    "    year=row.first_arrest_year\n",
    "    year=str(2010)\n",
    "    offender=row.first_arrest_NoUnique\n",
    "    networkFeat=networkFeatures[year]\n",
    "    sperson=str(offender)\n",
    "    try:\n",
    "        eigen=networkFeat['eigen'][sperson]\n",
    "        degree=networkFeat['degree'][sperson]\n",
    "        clus=networkFeat['clus'][sperson]\n",
    "        closeness=networkFeat['closeness'][sperson]\n",
    "    except:\n",
    "        eigen=0\n",
    "        degree=0\n",
    "        clus=0\n",
    "        closeness=0\n",
    "        print (sperson)\n",
    "    return eigen,degree,clus,closeness\n",
    "\n",
    "def addNetworkFeatures3(co_offending_table,trainsub):\n",
    "    networkFeatures=json.load(open('networkFeatures2010.json','rb'))\n",
    "    df=pd.DataFrame(trainsub.first_arrest_NoUnique)\n",
    "    df['eigen2010'],df['degree2010'],df['clus2010'],df['closeness2010']=zip(*trainsub[['first_arrest_year','first_arrest_NoUnique','first_arrest_SeqE']].apply(lambda x: getNetworkVar3(x,networkFeatures),axis=1))\n",
    "    df.to_csv('networkFeatures3_23.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms for Regresion Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runLogPrediction(training_data,testing_data,xVars):\n",
    "    X_df = training_data[xVars]\n",
    "    # gives us dummy variables\n",
    "    X_df = pd.get_dummies(X_df)\n",
    "    X = X_df.as_matrix()\n",
    "\n",
    "    Y_df = training_data[['arrested_again']]\n",
    "    Y = Y_df.as_matrix()\n",
    "    Y = Y.ravel()\n",
    "    baseline_model = LogisticRegression(penalty='l2')\n",
    "    baseline_model.fit(X, Y)\n",
    "    \n",
    "    res = np.argsort(abs(baseline_model.coef_))[0]\n",
    "    res = res[::-1]\n",
    "    print('bias: %f' % baseline_model.intercept_)\n",
    "    for coeff_index in res[0:20]:\n",
    "        value = baseline_model.coef_[0][coeff_index]\n",
    "        name = X_df.columns[coeff_index]\n",
    "        print('coefficient: %s  | value: %f' % (name, value))\n",
    "    print('Accuracy score of {} during training'.format(baseline_model.score(X, Y)))\n",
    "    Yprob=baseline_model.predict_proba(X)[:,1]\n",
    "    roc=roc_auc_score(Y, Yprob)\n",
    "    print ('ROC score of {} during training'.format(roc))\n",
    "    X_test_df = testing_data[xVars]\n",
    "    # gives us dummy variables\n",
    "    X_test_df = pd.get_dummies(X_test_df)\n",
    "    X_test = X_test_df.as_matrix()\n",
    "    Y_test_df = testing_data[['arrested_again']]\n",
    "    Y_test= Y_test_df.as_matrix()\n",
    "    Y_test = Y_test.ravel()\n",
    "    testscore=baseline_model.score(X_test, Y_test)\n",
    "    Ytestprob=baseline_model.predict_proba(X_test)[:,1]\n",
    "    testroc=roc_auc_score(Y_test, Ytestprob)\n",
    "    return testscore,testroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runRandomForest(training_data,testing_data,xVars,n,k):\n",
    "    X_df = training_data[xVars]\n",
    "    # gives us dummy variables\n",
    "    X_df = pd.get_dummies(X_df)\n",
    "    X = X_df.as_matrix()\n",
    "\n",
    "    Y_df = training_data[['arrested_again']]\n",
    "    Y = Y_df.as_matrix()\n",
    "    Y = Y.ravel()\n",
    "    \n",
    "    classifer=skl.ensemble.RandomForestClassifier(n_estimators=n,max_features=k,bootstrap=True,oob_score=False)\n",
    "    classifer.fit(X,Y)\n",
    "    \n",
    "    print('Accuracy score of {} during training'.format(classifer.score(X, Y)))\n",
    "    Yprob=classifer.predict_proba(X)[:,1]\n",
    "    roc=roc_auc_score(Y, Yprob)\n",
    "    print ('ROC score of {} during training'.format(roc))\n",
    "    X_test_df = testing_data[xVars]\n",
    "    # gives us dummy variables\n",
    "    X_test_df = pd.get_dummies(X_test_df)\n",
    "    X_test = X_test_df.as_matrix()\n",
    "    Y_test_df = testing_data[['arrested_again']]\n",
    "    Y_test= Y_test_df.as_matrix()\n",
    "    Y_test = Y_test.ravel()\n",
    "    testscore=classifer.score(X_test, Y_test)\n",
    "    Ytestprob=classifer.predict_proba(X_test)[:,1]\n",
    "    testroc=roc_auc_score(Y_test, Ytestprob)\n",
    "    return testscore,testroc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kFOlD(k,trainingdata,predictor,xVars):\n",
    "    df = shuffle(trainingdata)\n",
    "    def partition(df, n): \n",
    "        division = len(df) / float(n) \n",
    "        return [ df.iloc[int(round(division * i)): int(round(division * (i + 1))),:] for i in xrange(n) ]\n",
    "    dfs=partition(df,k)\n",
    "    testscores=[]\n",
    "    aucscores=[]\n",
    "    for i in range(k):\n",
    "        training=copy.deepcopy(dfs)\n",
    "        del training[i]\n",
    "        training=pd.concat(training)\n",
    "        validation=dfs[i]\n",
    "        testscore,aucscore=predictor(training,validation,xVars)\n",
    "        testscores.append(testscore)\n",
    "        aucscores.append(aucscore)\n",
    "    print('The average validation score is {}'.format(np.mean(testscores)))\n",
    "    print('The average validation AUC is {}'.format(np.mean(aucscores)))\n",
    "    return testscores,aucscores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Subset of Data with network info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first-second arrest\n",
    "training_subset=training_data[(training_data.arrestedWithAdult==1)]\n",
    "networkData1=pd.read_csv('networkFeatures1.csv')\n",
    "networkData2=pd.read_csv('networkFeatures2.csv')\n",
    "networkData3=pd.read_csv('networkFeatures3.csv')\n",
    "training_subset=pd.merge(training_subset,networkData1,how='left',on='first_arrest_NoUnique')\n",
    "training_subset=pd.merge(training_subset,networkData2,how='left',on='first_arrest_NoUnique')\n",
    "training_subset=pd.merge(training_subset,networkData3,how='left',on='first_arrest_NoUnique')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#second-third arrest\n",
    "training_subset=training_data[(training_data.arrestedWithAdult==1)]\n",
    "networkData1=pd.read_csv('networkFeatures1_23.csv')\n",
    "networkData2=pd.read_csv('networkFeatures2_23.csv')\n",
    "networkData3=pd.read_csv('networkFeatures3_23.csv')\n",
    "training_subset=pd.merge(training_subset,networkData1,how='left',on='first_arrest_NoUnique')\n",
    "training_subset=pd.merge(training_subset,networkData2,how='left',on='first_arrest_NoUnique')\n",
    "training_subset=pd.merge(training_subset,networkData3,how='left',on='first_arrest_NoUnique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 percent of the people are arrested with someone and has networkinfo in this year[2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010]\n",
      "data size is 88416\n",
      "raw recidivism rate: 0.346736\n",
      "wild guess accuracy is 0.653264115092\n"
     ]
    }
   ],
   "source": [
    "year=[2003]\n",
    "year=[2003,2004,2005,2006,2007,2008,2009,2010]\n",
    "training_yearsub=training_subset[training_subset.first_arrest_year.isin(year)]\n",
    "siz=1.0*len(training_yearsub)/len(training_subset)\n",
    "print('{} percent of the people are arrested with someone and has networkinfo in this year{}'.format(siz,year))\n",
    "print ('data size is {}'.format(len(training_yearsub)))\n",
    "print('raw recidivism rate: %f' % (1.0*sum(training_yearsub.arrested_again)/len(training_yearsub.arrested_again)))\n",
    "print ('wild guess accuracy is {}'.format(1-1.0*sum(training_yearsub.arrested_again)/len(training_yearsub.arrested_again)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xVars=['first_arrest_SEXE','first_arrest_NCD1', 'first_arrest_MUN', 'first_arrest_ED1','age_cat']\n",
    "testscore,testroc=runLogPrediction(training_yearsub,training_yearsub,xVars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xVars=['first_arrest_SEXE','first_arrest_NCD1', 'first_arrest_MUN', 'first_arrest_ED1','age_cat']\n",
    "+['arrestedWithAdult','arrestedWithYouth']\n",
    "testscore,testroc=runLogPrediction(training_yearsub,training_yearsub,xVars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xVars=['first_arrest_SEXE','first_arrest_NCD1', 'first_arrest_MUN', 'first_arrest_ED1','age_cat']\n",
    "+['eigen','degree','clus','closeness'] \n",
    "testscore,testroc=runLogPrediction(training_yearsub,training_yearsub,xVars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xVars=['first_arrest_SEXE','first_arrest_NCD1', 'first_arrest_MUN', 'first_arrest_ED1','age_cat']\n",
    "+['eigen2010','degree2010','clus2010','closeness2010']\n",
    "testscore,testroc=runLogPrediction(training_yearsub,training_yearsub,xVars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xVars=['first_arrest_SEXE','first_arrest_NCD1', 'first_arrest_MUN', 'first_arrest_ED1','age_cat']\n",
    "+['Neigen','Ndegree','Nclus','Ncloseness','Nclus2',] \n",
    "testscore,testroc=runLogPrediction(training_yearsub,training_yearsub,xVars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K folds for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fold=3\n",
    "xVars=['first_arrest_SEXE','first_arrest_NCD1', 'first_arrest_MUN', 'first_arrest_ED1','age_cat']\n",
    "testscores,aucscores=kFOlD(fold,training_yearsub,runLogPrediction,xVars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fold=3\n",
    "xVars=['first_arrest_SEXE','first_arrest_NCD1', 'first_arrest_MUN', 'first_arrest_ED1','age_cat']\n",
    "+['eigen','degree','clus','closeness'] \n",
    "testscores,aucscores=kFOlD(fold,training_yearsub,runLogPrediction,xVars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of 0.865816141875 during training\n",
      "ROC score of 0.94417564891 during training\n"
     ]
    }
   ],
   "source": [
    "n=100\n",
    "k=10\n",
    "xVars=['first_arrest_SEXE','first_arrest_NCD1', 'first_arrest_MUN', 'first_arrest_ED1','age_cat']\n",
    "testscore,testroc=runRandomForest(training_yearsub,training_yearsub,xVars,n,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of 0.86961635903 during training\n",
      "ROC score of 0.947086961147 during training\n"
     ]
    }
   ],
   "source": [
    "n=100\n",
    "k=10\n",
    "xVars=['first_arrest_SEXE','first_arrest_NCD1', 'first_arrest_MUN', 'first_arrest_ED1','age_cat']+['arrestedWithAdult','arrestedWithYouth']\n",
    "testscore,testroc=runRandomForest(training_yearsub,training_yearsub,xVars,n,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of 0.936029677886 during training\n",
      "ROC score of 0.986554217255 during training\n"
     ]
    }
   ],
   "source": [
    "n=100\n",
    "k=10\n",
    "xVars=['first_arrest_SEXE','first_arrest_NCD1', 'first_arrest_MUN', 'first_arrest_ED1','age_cat']+['eigen','degree','clus','closeness'] \n",
    "testscore,testroc=runRandomForest(training_yearsub,training_yearsub,xVars,n,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=100\n",
    "k=10\n",
    "xVars=['first_arrest_SEXE','first_arrest_NCD1', 'first_arrest_MUN', 'first_arrest_ED1','age_cat']\n",
    "+['eigen2010','degree2010','clus2010','closeness2010']\n",
    "testscore,testroc=runRandomForest(training_yearsub,training_yearsub,xVars,n,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of 0.936663047412 during training\n",
      "ROC score of 0.986667366575 during training\n"
     ]
    }
   ],
   "source": [
    "n=100\n",
    "k=10\n",
    "xVars=['first_arrest_SEXE','first_arrest_NCD1', 'first_arrest_MUN', 'first_arrest_ED1','age_cat']+['Neigen','Ndegree','Nclus','Ncloseness','Nclus2'] \n",
    "testscore,testroc=runRandomForest(training_yearsub,training_yearsub,xVars,n,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K folds for Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fold=3\n",
    "n=100\n",
    "k=5\n",
    "xVars=['first_arrest_SEXE','first_arrest_NCD1', 'first_arrest_MUN', 'first_arrest_ED1','age_cat']\n",
    "testscores,aucscores=kFOlD(fold,training_yearsub,lambda x,y,z: runRandomForest(x,y,z,n,k),xVars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fold=3\n",
    "n=100\n",
    "k=5\n",
    "xVars=['first_arrest_SEXE','first_arrest_NCD1', 'first_arrest_MUN', 'first_arrest_ED1','age_cat']\n",
    "+['eigen','degree','clus','closeness'] \n",
    "testscores,aucscores=kFOlD(fold,training_yearsub,lambda x,y,z: runRandomForest(x,y,z,n,k),xVars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_subset['ones']=[1 for i in range(len(training_subset))]\n",
    "xVars=['first_arrest_SEXE','first_arrest_NCD1', 'first_arrest_MUN', 'first_arrest_ED1','age_cat']\n",
    "xVars=['ones']\n",
    "#testscore,testroc=runLogPrediction(training_yearsub,training_yearsub,xVars)\n",
    "testscore,testroc=runRandomForest(training_yearsub,training_yearsub,xVars,n,k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
